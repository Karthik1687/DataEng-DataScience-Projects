{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Incremental Load\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Implement a logic to load incremental exchange rates data on a daily basis from RBA website into warehouse.\n",
    "\n",
    "## Psuedo code\n",
    "\n",
    "1. Extract data from RBA website\n",
    "2. Cleanse the data and add extra ETL specific fields if needed.\n",
    "3. Establish spark session\n",
    "4. Load the created dataframe into the table.\n",
    "5. The similar approach can be used to load into any table by establishing db connection and writing the dataframe\n",
    "   df.write.options API in pyspark can be used to insert data into Hive/HDFS in various formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Importing Necessary Modules and Assignments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from datetime import datetime\n",
    "from pyspark import SparkContext as sc\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RBA Incremental Exchange Rates\").getOrCreate()\n",
    "log4jLogger = spark._jvm.org.apache.log4j \n",
    "log = log4jLogger.LogManager.getLogger(__name__) \n",
    "\n",
    "def get_data(load_type):\n",
    "    if load_type == 'historical':\n",
    "        data=pd.read_csv(\"/Users/karthikdivya/Desktop/Personal/PySpark/historical_rates.csv\", header=10)\n",
    "        return data\n",
    "    elif load_type == 'delta':\n",
    "        # Data Url can extract the latest records when it is run and load into the table.\n",
    "        data_url = 'https://www.rba.gov.au/statistics/frequency/exchange-rates.html#latest-exchange-rates'\n",
    "        log.info(\"Accessing the site {site} and extracting exchange rates data\".format(site=data_url))\n",
    "        tables = pd.read_html(data_url)\n",
    "        latest_exchange_rates = tables[0]\n",
    "        return latest_exchange_rates\n",
    "        log.info(\"Downloaded data. Data Cleansing in progress...\")\n",
    "\n",
    "def get_cleansed_data(latest_exchange_rates):\n",
    "    \n",
    "    latest_fx_rate_date = datetime.strptime(latest_exchange_rates.columns[-1], '%d %b %Y').strftime('%Y-%m-%d')\n",
    "    prev_fx_rate_date = datetime.strptime(latest_exchange_rates.columns[-2], '%d %b %Y').strftime('%Y-%m-%d')\n",
    "    oldest_fx_rate_date = datetime.strptime(latest_exchange_rates.columns[-3], '%d %b %Y').strftime('%Y-%m-%d')\n",
    "    latest_exchange_rates.columns = ['currency', 'oldest_fx_rate_date', 'prev_fx_rate_date', 'latest_fx_rate_date']\n",
    "\n",
    "    # Additional Columns - Currency+exch_rate_date combination would be unique\n",
    "    load_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    base_currency = 'AUD'\n",
    "    latest_exchange_rates['exch_rate_date'] = latest_fx_rate_date # The date for which the exchange rates apply\n",
    "    latest_exchange_rates['load_date'] = load_date # The date on which the load happened\n",
    "    latest_exchange_rates['base_currency'] = str(base_currency) # Currency against which these rates are reported\n",
    "\n",
    "    # Drop the previous exchange rates column\n",
    "    latest_exchange_rates.drop(['oldest_fx_rate_date', 'prev_fx_rate_date'], axis=1, inplace=True)\n",
    "    \n",
    "    return latest_exchange_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering an empty table for the initial run\n",
    "\n",
    "1. I have included this step to demonstrate the working in displaying the spark temporary table.\n",
    "2. Ideally we would create the table in database before hand and append data during the regular run.\n",
    "3. This step will not be needed and can be skipped at that time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------+---------+-------------+\n",
      "|currency|latest_fx_rate|exch_rate_date|load_date|base_currency|\n",
      "+--------+--------------+--------------+---------+-------------+\n",
      "+--------+--------------+--------------+---------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the fields and defining datatypes for the table\n",
    "field = [StructField(\"currency\",StringType(), True),StructField(\"latest_fx_rate\", StringType(), True),\\\n",
    "StructField(\"exch_rate_date\", StringType(), True),StructField(\"load_date\", StringType(), True),\\\n",
    "StructField(\"base_currency\", StringType(), True)]\n",
    "schema = StructType(field)\n",
    "\n",
    "# Creating empty dataframe based on the above schema\n",
    "empty_df = spark.createDataFrame([], schema)\n",
    "\n",
    "# Creating writer object for the above dataframe\n",
    "df_writer=pyspark.sql.DataFrameWriter(empty_df)\n",
    " \n",
    "# We can use this to do full load every time if needed or use mode=overwrite if the table is partitioned.\n",
    "df_writer.saveAsTable(\"daily_exchange_rates\", mode=\"append\")\n",
    "\n",
    "# create_df\n",
    "rows=spark.sql(\"select * from daily_exchange_rates\")\n",
    "rows.show(100)\n",
    "rows.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading using Spark. \n",
    "\n",
    "Create a dataframe and save this dataframe as a table for querying. For the purpose of this exercise, this is being stored as a spark temporary table in this notebook which would ideally be in spark_warehouse (/Users/karthikdivya/Desktop/Personal/PySpark/spark-warehouse) that is locally created. There would be a file that would be created with the name of the table. Incase if we want to drop the table, this file can be deleted and the script can be rerun to load the data.\n",
    "\n",
    "Ideally for production scenario, this would be written into hive directly or in any file formats (parquet, orc, etc., ) in HDFS directly. There are connectors in pyspark to connect to other databases as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------+---------+-------------+\n",
      "|currency|latest_fx_rate|exch_rate_date|load_date|base_currency|\n",
      "+--------+--------------+--------------+---------+-------------+\n",
      "+--------+--------------+--------------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute the below steps to clean up the table before re-running\n",
    "spark.sql(\"truncate table daily_exchange_rates\")\n",
    "rerun_rows=spark.sql(\"select * from daily_exchange_rates\")\n",
    "rerun_rows.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               currency  latest_fx_rate exch_rate_date   load_date  \\\n",
      "0  United States dollar          0.7093     2019-02-22  2019-02-23   \n",
      "1      Chinese renminbi          4.7736     2019-02-22  2019-02-23   \n",
      "2          Japanese yen         78.5800     2019-02-22  2019-02-23   \n",
      "3         European euro          0.6257     2019-02-22  2019-02-23   \n",
      "4      South Korean won        798.4900     2019-02-22  2019-02-23   \n",
      "\n",
      "  base_currency  \n",
      "0           AUD  \n",
      "1           AUD  \n",
      "2           AUD  \n",
      "3           AUD  \n",
      "4           AUD  \n"
     ]
    }
   ],
   "source": [
    "# Main Program Starts here\n",
    "# load_type can be 'historical' or 'delta' if it need to be parameterized.\n",
    "load_type = 'delta' \n",
    "\n",
    "# Call the function to get the data based on the load type. For delta, it will get the data directly from the RBA URL\n",
    "# and get the data cleansed. For full, it would be from the downloaded csv file stored in the local path. Assumption is\n",
    "# that full load will be initial one-off and the delta load will continue from there on a daily basis.\n",
    "data = get_data(load_type)\n",
    "cleansed_data = get_cleansed_data(data)\n",
    "cleansed_data.rename(columns={'latest_fx_rate_date':'latest_fx_rate'}, inplace=True)\n",
    "\n",
    "# Checking the cleansed data. Just for debugging purpose. Not needed in production code.\n",
    "print(cleansed_data.head(5))\n",
    "\n",
    "log.info(\"Data Cleansing Finished. Building spark session and performing validation before insertion..\")\n",
    "\n",
    "# Data validation. We do not want the rates for the same date to be loaded twice into our table.\n",
    "# The below rule ensures that the load will be triggered only when the latest record from table is not\n",
    "# equal to the latest information from RBA data downloaded\n",
    "max_exch_rate_date_df=spark.sql(\"select max(exch_rate_date) as max_exch_date from daily_exchange_rates\")\n",
    "max_exch_rate_date = max_exch_rate_date_df.collect()[0]['max_exch_date']\n",
    "if not (max_exch_rate_date==cleansed_data['exch_rate_date'].unique()[0]):\n",
    "    log.info(\"The latest date available in table is not equal to the latest from RBA. So, inserting new RBA rates !!!\")\n",
    "    cleansed_data_df=spark.createDataFrame(cleansed_data)\n",
    "    \n",
    "    # Create dataframe writer object for the cleansed exchange rates data frame\n",
    "    df_writer=pyspark.sql.DataFrameWriter(cleansed_data_df) \n",
    "\n",
    "    # Saving the data as table. We can include the code to write to database here.\n",
    "    # We can use this to do full load every time if needed or use mode=overwrite if the table is partitioned.\n",
    "    df_writer.saveAsTable(\"daily_exchange_rates\", mode=\"append\")\n",
    "    load_count = cleansed_data_df.count()\n",
    "    print('Loading is completed. Loaded a total of {load_count} records'.format(load_count=load_count))\n",
    "    log.info('Loading is completed. Loaded a total of {load_count} records'.format(load_count=load_count))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------+----------+-------------+\n",
      "|            currency|latest_fx_rate|exch_rate_date| load_date|base_currency|\n",
      "+--------------------+--------------+--------------+----------+-------------+\n",
      "|Papua New Guinea ...|        2.3882|    2019-02-22|2019-02-23|          AUD|\n",
      "|         Swiss franc|        0.7102|    2019-02-22|2019-02-23|          AUD|\n",
      "|United Arab Emira...|        2.6049|    2019-02-22|2019-02-23|          AUD|\n",
      "|     Canadian dollar|         0.939|    2019-02-22|2019-02-23|          AUD|\n",
      "|Trade-weighted In...|          60.5|    2019-02-22|2019-02-23|          AUD|\n",
      "|Special Drawing R...|        0.5093|    2019-02-22|2019-02-23|          AUD|\n",
      "|United States dollar|        0.7093|    2019-02-22|2019-02-23|          AUD|\n",
      "|    Chinese renminbi|        4.7736|    2019-02-22|2019-02-23|          AUD|\n",
      "|        Japanese yen|         78.58|    2019-02-22|2019-02-23|          AUD|\n",
      "|       European euro|        0.6257|    2019-02-22|2019-02-23|          AUD|\n",
      "|    South Korean won|        798.49|    2019-02-22|2019-02-23|          AUD|\n",
      "|    Singapore dollar|         0.961|    2019-02-22|2019-02-23|          AUD|\n",
      "|  New Zealand dollar|        1.0476|    2019-02-22|2019-02-23|          AUD|\n",
      "|   UK pound sterling|        0.5444|    2019-02-22|2019-02-23|          AUD|\n",
      "|   Malaysian ringgit|        2.8961|    2019-02-22|2019-02-23|          AUD|\n",
      "|           Thai baht|         22.21|    2019-02-22|2019-02-23|          AUD|\n",
      "|   Indonesian rupiah|        9980.0|    2019-02-22|2019-02-23|          AUD|\n",
      "|        Indian rupee|          50.5|    2019-02-22|2019-02-23|          AUD|\n",
      "|   New Taiwan dollar|         21.85|    2019-02-22|2019-02-23|          AUD|\n",
      "|     Vietnamese dong|       16466.0|    2019-02-22|2019-02-23|          AUD|\n",
      "|    Hong Kong dollar|        5.5668|    2019-02-22|2019-02-23|          AUD|\n",
      "+--------------------+--------------+--------------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows=spark.sql(\"select * from daily_exchange_rates\")\n",
    "rows.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerun the above script and check the count\n",
    "\n",
    "As long the maximum of exch_rate_date equals the latest exchange rate from the downloaded daily exchange rates table, it would not load duplicate data irrespective of how many ever times it was re run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rerun_rows=spark.sql(\"select * from daily_exchange_rates\")\n",
    "rerun_rows.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical data load\n",
    "\n",
    "## Steps to Cleanse the table before historical loading\n",
    "\n",
    "\"daily_exchange_rates\" is the table that is created for storing the data. It is a good idea to truncate the table before the historical load and make sure the table does not have any data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------+---------+-------------+\n",
      "|currency|latest_fx_rate|exch_rate_date|load_date|base_currency|\n",
      "+--------+--------------+--------------+---------+-------------+\n",
      "+--------+--------------+--------------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"truncate table daily_exchange_rates\")\n",
    "rerun_rows=spark.sql(\"select * from daily_exchange_rates\")\n",
    "rerun_rows.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading is completed. Loaded a total of 288 records for United States dollar\n",
      "Loading is completed. Loaded a total of 288 records for Trade-weighted Index (4pm)\n",
      "Loading is completed. Loaded a total of 288 records for Chinese renminbi\n",
      "Loading is completed. Loaded a total of 288 records for Japanese yen\n",
      "Loading is completed. Loaded a total of 288 records for European euro\n",
      "Loading is completed. Loaded a total of 288 records for South Korean won\n",
      "Loading is completed. Loaded a total of 288 records for UK pound sterling\n",
      "Loading is completed. Loaded a total of 288 records for Singapore dollar\n",
      "Loading is completed. Loaded a total of 288 records for Indian rupee\n",
      "Loading is completed. Loaded a total of 288 records for Thai baht\n",
      "Loading is completed. Loaded a total of 288 records for New Zealand dollar\n",
      "Loading is completed. Loaded a total of 288 records for New Taiwan dollar\n",
      "Loading is completed. Loaded a total of 288 records for Malaysian ringgit\n",
      "Loading is completed. Loaded a total of 288 records for Indonesian rupiah\n",
      "Loading is completed. Loaded a total of 288 records for Vietnamese dong\n",
      "Loading is completed. Loaded a total of 288 records for United Arab Emirates dirham\n",
      "Loading is completed. Loaded a total of 288 records for Papua New Guinea kina\n",
      "Loading is completed. Loaded a total of 288 records for Hong Kong dollar\n",
      "Loading is completed. Loaded a total of 288 records for Canadian dollar\n",
      "Loading is completed. Loaded a total of 288 records for Swiss franc\n",
      "Loading is completed. Loaded a total of 288 records for Special Drawing Right\n"
     ]
    }
   ],
   "source": [
    "# Main Program Starts here\n",
    "# load_type can be 'historical' or 'delta' if it need to be parameterized.\n",
    "load_type = 'historical' \n",
    "currency_dict={'FXRUSD': 'United States dollar',\n",
    "               'FXRCR': 'Chinese renminbi',\n",
    "               'FXRJY': 'Japanese yen',\n",
    "               'FXREUR': 'European euro',\n",
    "               'FXRSKW': 'South Korean won',\n",
    "               'FXRUKPS': 'UK pound sterling',\n",
    "               'FXRSD': 'Singapore dollar',\n",
    "               'FXRIRE': 'Indian rupee',\n",
    "               'FXRTB': 'Thai baht',\n",
    "               'FXRNZD': 'New Zealand dollar',\n",
    "               'FXRNTD': 'New Taiwan dollar',\n",
    "               'FXRMR': 'Malaysian ringgit',\n",
    "               'FXRIR': 'Indonesian rupiah',\n",
    "               'FXRVD': 'Vietnamese dong',\n",
    "               'FXRUAED': 'United Arab Emirates dirham',\n",
    "               'FXRPNGK': 'Papua New Guinea kina',\n",
    "               'FXRHKD': 'Hong Kong dollar',\n",
    "               'FXRCD': 'Canadian dollar',\n",
    "               'FXRSF': 'Swiss franc',\n",
    "               'FXRSDR': 'Special Drawing Right',\n",
    "               'FXRTWI': 'Trade-weighted Index (4pm)'}\n",
    "\n",
    "data=get_data(load_type)\n",
    "\n",
    "# Dropping unwanted columns and extracting only relevant rows\n",
    "data.drop('FXRPHP', axis=1, inplace=True)\n",
    "data.drop('FXRSARD', axis=1, inplace=True)\n",
    "#print(data.columns)\n",
    "cleansed_data=data.iloc[:288, 0:22]\n",
    "\n",
    "# Renaming the column names as per the mapping dictionary currency_dict\n",
    "cleansed_data.rename(columns=currency_dict, inplace=True)\n",
    "\n",
    "# Extracting the currency column names for looping\n",
    "currency_columns = cleansed_data.columns[1:]\n",
    "\n",
    "# Additional columns\n",
    "load_date = datetime.now().strftime('%Y-%m-%d')\n",
    "base_currency = 'AUD'\n",
    "\n",
    "for currency in currency_columns:\n",
    "    hist_currency_data=cleansed_data.loc[:, ['Series ID', currency]]\n",
    "    hist_currency_data['currency']=currency\n",
    "    hist_currency_data['load_date'] = load_date\n",
    "    hist_currency_data['base_currency'] = str(base_currency)\n",
    "    columns_mapping={'Series ID': 'exch_rate_date',\n",
    "                     currency: 'latest_fx_rate' ,\n",
    "                     'currency': 'currency',\n",
    "                     'load_date': 'load_date',\n",
    "                     'base_currency': 'base_currency'}\n",
    "    hist_currency_data.rename(columns=columns_mapping, inplace=True)\n",
    "    hist_currency_data_df=spark.createDataFrame(hist_currency_data)\n",
    "    \n",
    "    # Create dataframe writer object for the cleansed historical exchange rates data frame\n",
    "    df_hist_writer=pyspark.sql.DataFrameWriter(hist_currency_data_df) \n",
    "\n",
    "    # Saving the data as table. We can include the code to write to database here.\n",
    "    df_hist_writer.saveAsTable(\"daily_exchange_rates\", mode=\"append\")\n",
    "    load_count = hist_currency_data_df.count()\n",
    "    print('Loading is completed. Loaded a total of {hist_currency_data_df} records for {currency}'.format(hist_currency_data_df=load_count, currency=currency))\n",
    "    log.info('Loading is completed. Loaded a total of {hist_currency_data_df} records'.format(hist_currency_data_df=load_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Vaidation for Historical Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_load_rows=spark.sql(\"select * from daily_exchange_rates where currency='United States dollar'\\\n",
    "                         order by exch_rate_date asc\")\n",
    "hist_load_rows.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------+----------+-------------+\n",
      "|            currency|latest_fx_rate|exch_rate_date| load_date|base_currency|\n",
      "+--------------------+--------------+--------------+----------+-------------+\n",
      "|United States dollar|        0.7407|   01-Aug-2018|2019-02-23|          AUD|\n",
      "|United States dollar|        0.8044|   01-Feb-2018|2019-02-23|          AUD|\n",
      "|United States dollar|        0.7241|   01-Feb-2019|2019-02-23|          AUD|\n",
      "|United States dollar|        0.7541|   01-Jun-2018|2019-02-23|          AUD|\n",
      "|United States dollar|        0.7731|   01-Mar-2018|2019-02-23|          AUD|\n",
      "|United States dollar|         0.754|   01-May-2018|2019-02-23|          AUD|\n",
      "|United States dollar|         0.713|   01-Nov-2018|2019-02-23|          AUD|\n",
      "|United States dollar|        0.7385|   02-Aug-2018|2019-02-23|          AUD|\n",
      "|United States dollar|        0.7997|   02-Feb-2018|2019-02-23|          AUD|\n",
      "|United States dollar|        0.7837|   02-Jan-2018|2019-02-23|          AUD|\n",
      "|United States dollar|         0.702|   02-Jan-2019|2019-02-23|          AUD|\n",
      "|United States dollar|        0.7382|   02-Jul-2018|2019-02-23|          AUD|\n",
      "|United States dollar|        0.7756|   02-Mar-2018|2019-02-23|          AUD|\n",
      "|United States dollar|        0.7503|   02-May-2018|2019-02-23|          AUD|\n",
      "|United States dollar|        0.7243|   02-Nov-2018|2019-02-23|          AUD|\n",
      "|United States dollar|          0.72|   02-Oct-2018|2019-02-23|          AUD|\n",
      "|United States dollar|        0.7685|   03-Apr-2018|2019-02-23|          AUD|\n",
      "|United States dollar|         0.737|   03-Aug-2018|2019-02-23|          AUD|\n",
      "|United States dollar|        0.7367|   03-Dec-2018|2019-02-23|          AUD|\n",
      "|United States dollar|        0.7816|   03-Jan-2018|2019-02-23|          AUD|\n",
      "+--------------------+--------------+--------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist_load_rows.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6048"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_load_rows=spark.sql(\"select * from daily_exchange_rates\")\n",
    "hist_load_rows.count() # 21 currencies * 288 rows per currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
