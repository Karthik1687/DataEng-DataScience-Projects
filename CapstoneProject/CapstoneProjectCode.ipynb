{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['info', 'imsave', 'imshow', 'imread']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.misc import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# Keras and Tensorflow related imports\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "# To display Images\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "%pylab inline\n",
    "\n",
    "# Control Randomness\n",
    "seed = 222\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result\n",
    "Populating the interactive namespace from numpy and matplotlib\n",
    "Using TensorFlow backend.\n",
    "/usr/local/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['imread', 'imshow', 'source', 'imsave', 'who', 'info']\n",
    "`%matplotlib` prevents importing * from pylab and numpy\n",
    "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Directory Setup\n",
    "# project_dir = os.getcwd()\n",
    "# project_dir = '/Users/krishnamoorthypitchumani/PycharmProjects/Udacity Machine Learning/Udacity-Machine-Learning/projects/CapstoneProject'\n",
    "project_dir = '/floyd/input/ageclassification'\n",
    "train_img_dir = os.path.join(project_dir,'Data','train','Train')\n",
    "test_img_dir = os.path.join(project_dir,'Data','test','Test')\n",
    "train_data_dir = os.path.join(project_dir,'Data','train')\n",
    "test_data_dir = os.path.join(project_dir,'Data','test')\n",
    "\n",
    "# Get ImageNames\n",
    "def get_image_name(n):\n",
    "    yield str(n) + '.jpg'\n",
    "\n",
    "# View random number of images\n",
    "def get_images(n):\n",
    "    for i in range(n):\n",
    "        index = random.randint(0,10000)\n",
    "        yield str(index) + '.jpg'\n",
    "        \n",
    "# Display Images\n",
    "def display_images(n):\n",
    "    image_list = [os.path.join(train_img_dir, image) for image in get_images(n)]\n",
    "    print(image_list)\n",
    "\n",
    "# Display Image from directory\n",
    "def display(img_array):\n",
    "    for img in img_array:\n",
    "        print(img.shape)\n",
    "        plt.imshow(img)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>377.jpg</td>\n",
       "      <td>MIDDLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17814.jpg</td>\n",
       "      <td>YOUNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21283.jpg</td>\n",
       "      <td>MIDDLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16496.jpg</td>\n",
       "      <td>YOUNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4487.jpg</td>\n",
       "      <td>MIDDLE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID   Class\n",
       "0    377.jpg  MIDDLE\n",
       "1  17814.jpg   YOUNG\n",
       "2  21283.jpg  MIDDLE\n",
       "3  16496.jpg   YOUNG\n",
       "4   4487.jpg  MIDDLE"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Input Data\n",
    "train = pd.read_csv(os.path.join(train_data_dir, 'train.csv'))\n",
    "test = pd.read_csv(os.path.join(test_data_dir, 'test.csv'))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "from scipy.misc import imresize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import cv2\n",
    "\n",
    "# Utility function to read images as arrays\n",
    "def transform_image(img_path, mode = 'color', resize = False, size = 32):\n",
    "    '''\n",
    "    Default mode is : color(BGR) --> color(RGB)\n",
    "    Other modes allowed are : 'grayscale' and 'include_opacity'\n",
    "    '''\n",
    "    if mode == 'grayscale':\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "#     elif mode == 'include_opacity':\n",
    "#         img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "    else:\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  \n",
    "    if resize == True:\n",
    "        img = cv2.resize(img, (size, size))\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Resize_normalize images\n",
    "def resize_images(dataset, img_dir):\n",
    "    resized_array = []\n",
    "    counter = 0\n",
    "    for image_id in dataset.ID:\n",
    "        img_path = os.path.join(img_dir, image_id)\n",
    "        image = transform_image(img_path, resize=True, size = 64)\n",
    "        image = image.astype('float32')\n",
    "        resized_array.append(image)\n",
    "        if counter == 19000:\n",
    "            break\n",
    "        counter += 1\n",
    "    final_resized = np.stack(resized_array)\n",
    "    return final_resized/255\n",
    "\n",
    "# Encode the output class variable\n",
    "def encode_feature(feature):\n",
    "    lb = LabelEncoder()\n",
    "    transformed_feature = lb.fit_transform(feature)\n",
    "    transformed_feature = keras.utils.np_utils.to_categorical(transformed_feature)\n",
    "    return transformed_feature\n",
    "\n",
    "train_x = resize_images(train, train_img_dir)\n",
    "test_x = resize_images(test, test_img_dir)\n",
    "train_y = encode_feature(train['Class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result\n",
    "\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "\n",
    "=================================================================\n",
    "flatten_2 (Flatten)          (None, 12288)             0         \n",
    "_________________________________________________________________\n",
    "dense_3 (Dense)              (None, 500)               6144500   \n",
    "_________________________________________________________________\n",
    "dense_4 (Dense)              (None, 3)                 1503      \n",
    "\n",
    "=================================================================\n",
    "Total params: 6,146,003\n",
    "Trainable params: 6,146,003\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "Train on 15200 samples, validate on 3801 samples\n",
    "\n",
    "Epoch 1/20\n",
    "15200/15200 [==============================] - 2s 133us/step - loss: 1.4581 - acc: 0.4446 - val_loss: 0.9266 - val_acc: 0.5454\n",
    "\n",
    "Epoch 2/20\n",
    "15200/15200 [==============================] - 1s 73us/step - loss: 0.9205 - acc: 0.5569 - val_loss: 0.9037 - val_acc: 0.5556\n",
    "\n",
    "Epoch 3/20\n",
    "15200/15200 [==============================] - 1s 73us/step - loss: 0.8990 - acc: 0.5762 - val_loss: 0.8896 - val_acc: 0.5688\n",
    "\n",
    "Epoch 4/20\n",
    "15200/15200 [==============================] - 1s 73us/step - loss: 0.8950 - acc: 0.5689 - val_loss: 0.8833 - val_acc: 0.5712\n",
    "\n",
    "Epoch 5/20\n",
    "15200/15200 [==============================] - 1s 72us/step - loss: 0.8948 - acc: 0.5593 - val_loss: 0.8896 - val_acc: 0.5638\n",
    "\n",
    "Epoch 6/20\n",
    "15200/15200 [==============================] - 1s 71us/step - loss: 0.8724 - acc: 0.5891 - val_loss: 0.8628 - val_acc: 0.5962\n",
    "\n",
    "Epoch 7/20\n",
    "15200/15200 [==============================] - 1s 72us/step - loss: 0.8631 - acc: 0.5941 - val_loss: 0.8845 - val_acc: 0.5688\n",
    "\n",
    "Epoch 8/20\n",
    "15200/15200 [==============================] - 1s 73us/step - loss: 0.8862 - acc: 0.5680 - val_loss: 0.8573 - val_acc: 0.5933\n",
    "\n",
    "Epoch 9/20\n",
    "15200/15200 [==============================] - 1s 71us/step - loss: 0.8654 - acc: 0.5863 - val_loss: 0.8552 - val_acc: 0.5927\n",
    "\n",
    "Epoch 10/20\n",
    "15200/15200 [==============================] - 1s 73us/step - loss: 0.8643 - acc: 0.5850 - val_loss: 0.8766 - val_acc: 0.5733\n",
    "\n",
    "Epoch 11/20\n",
    "15200/15200 [==============================] - 1s 71us/step - loss: 0.8559 - acc: 0.5884 - val_loss: 0.8380 - val_acc: 0.6125\n",
    "\n",
    "Epoch 12/20\n",
    "15200/15200 [==============================] - 1s 73us/step - loss: 0.8387 - acc: 0.6126 - val_loss: 0.8475 - val_acc: 0.6033\n",
    "\n",
    "Epoch 13/20\n",
    "15200/15200 [==============================] - 1s 74us/step - loss: 0.8771 - acc: 0.5678 - val_loss: 0.8649 - val_acc: 0.5804\n",
    "\n",
    "Epoch 14/20\n",
    "15200/15200 [==============================] - 1s 73us/step - loss: 0.8516 - acc: 0.5915 - val_loss: 0.8326 - val_acc: 0.6143\n",
    "\n",
    "Epoch 15/20\n",
    "15200/15200 [==============================] - 1s 73us/step - loss: 0.8316 - acc: 0.6178 - val_loss: 0.8383 - val_acc: 0.6083\n",
    "\n",
    "Epoch 16/20\n",
    "15200/15200 [==============================] - 1s 74us/step - loss: 0.8696 - acc: 0.5758 - val_loss: 0.8580 - val_acc: 0.5827\n",
    "\n",
    "Epoch 17/20\n",
    "15200/15200 [==============================] - 1s 72us/step - loss: 0.8401 - acc: 0.6045 - val_loss: 0.8272 - val_acc: 0.6172\n",
    "\n",
    "Epoch 18/20\n",
    "15200/15200 [==============================] - 1s 73us/step - loss: 0.8277 - acc: 0.6159 - val_loss: 0.8344 - val_acc: 0.6096\n",
    "\n",
    "Epoch 19/20\n",
    "15200/15200 [==============================] - 1s 72us/step - loss: 0.8468 - acc: 0.5929 - val_loss: 0.8495 - val_acc: 0.5925\n",
    "\n",
    "Epoch 20/20\n",
    "15200/15200 [==============================] - 1s 74us/step - loss: 0.8328 - acc: 0.6040 - val_loss: 0.8226 - val_acc: 0.6175\n",
    "\n",
    "<keras.callbacks.History at 0x7f55b92fada0>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>5859.jpg</td>\n",
       "      <td>YOUNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>17169.jpg</td>\n",
       "      <td>MIDDLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>6691.jpg</td>\n",
       "      <td>OLD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>537.jpg</td>\n",
       "      <td>MIDDLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>26003.jpg</td>\n",
       "      <td>MIDDLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>5069.jpg</td>\n",
       "      <td>MIDDLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>1330.jpg</td>\n",
       "      <td>MIDDLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>23418.jpg</td>\n",
       "      <td>MIDDLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>361.jpg</td>\n",
       "      <td>OLD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID   Class\n",
       "501   5859.jpg   YOUNG\n",
       "502  17169.jpg  MIDDLE\n",
       "503   6691.jpg     OLD\n",
       "504    537.jpg  MIDDLE\n",
       "505  26003.jpg  MIDDLE\n",
       "506   5069.jpg  MIDDLE\n",
       "507   1330.jpg  MIDDLE\n",
       "508  23418.jpg  MIDDLE\n",
       "509    361.jpg     OLD"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.Class[0:500]\n",
    "train[501:510]\n",
    "\n",
    "# train_x.shape\n",
    "# valid_x[0][0][0]\n",
    "# valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[[0.03921569, 0.03529412, 0.03529412],\n",
       "          [0.03529412, 0.03529412, 0.03529412],\n",
       "          [0.03137255, 0.03529412, 0.03921569],\n",
       "          ...,\n",
       "          [0.10980392, 0.07450981, 0.02745098],\n",
       "          [0.07843138, 0.07843138, 0.03921569],\n",
       "          [0.06666667, 0.07450981, 0.03921569]],\n",
       " \n",
       "         [[0.04313726, 0.03921569, 0.02745098],\n",
       "          [0.03529412, 0.03529412, 0.03529412],\n",
       "          [0.03137255, 0.03921569, 0.03529412],\n",
       "          ...,\n",
       "          [0.11764706, 0.08235294, 0.03529412],\n",
       "          [0.07843138, 0.07843138, 0.03921569],\n",
       "          [0.06666667, 0.07450981, 0.03921569]],\n",
       " \n",
       "         [[0.03921569, 0.03921569, 0.02352941],\n",
       "          [0.03529412, 0.03529412, 0.02745098],\n",
       "          [0.03137255, 0.03921569, 0.02745098],\n",
       "          ...,\n",
       "          [0.10588235, 0.07058824, 0.02745098],\n",
       "          [0.07843138, 0.07843138, 0.03921569],\n",
       "          [0.06666667, 0.07450981, 0.03921569]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.10196079, 0.10196079, 0.05490196],\n",
       "          [0.09411765, 0.08627451, 0.03529412],\n",
       "          [0.10196079, 0.08235294, 0.03137255],\n",
       "          ...,\n",
       "          [0.29411766, 0.1882353 , 0.10196079],\n",
       "          [0.22352941, 0.13725491, 0.05490196],\n",
       "          [0.16078432, 0.09019608, 0.01960784]],\n",
       " \n",
       "         [[0.07843138, 0.08627451, 0.03529412],\n",
       "          [0.08235294, 0.08627451, 0.03137255],\n",
       "          [0.09019608, 0.07058824, 0.01568628],\n",
       "          ...,\n",
       "          [0.29411766, 0.1882353 , 0.10196079],\n",
       "          [0.21568628, 0.12941177, 0.04705882],\n",
       "          [0.15294118, 0.08627451, 0.01176471]],\n",
       " \n",
       "         [[0.11764706, 0.1254902 , 0.07450981],\n",
       "          [0.09803922, 0.10196079, 0.04705882],\n",
       "          [0.07843138, 0.06666667, 0.01176471],\n",
       "          ...,\n",
       "          [0.2901961 , 0.18431373, 0.09803922],\n",
       "          [0.21176471, 0.1254902 , 0.04313726],\n",
       "          [0.14901961, 0.08235294, 0.00784314]]],\n",
       " \n",
       " \n",
       "        [[[0.00784314, 0.07450981, 0.00392157],\n",
       "          [0.01960784, 0.08627451, 0.01568628],\n",
       "          [0.03921569, 0.10588235, 0.04313726],\n",
       "          ...,\n",
       "          [0.23529412, 0.27058825, 0.26666668],\n",
       "          [0.19215687, 0.22745098, 0.22352941],\n",
       "          [0.16862746, 0.20784314, 0.20392157]],\n",
       " \n",
       "         [[0.02745098, 0.09411765, 0.02352941],\n",
       "          [0.03529412, 0.10196079, 0.03137255],\n",
       "          [0.04705882, 0.11372549, 0.05098039],\n",
       "          ...,\n",
       "          [0.22745098, 0.26666668, 0.25882354],\n",
       "          [0.18431373, 0.22352941, 0.21960784],\n",
       "          [0.16470589, 0.2       , 0.19607843]],\n",
       " \n",
       "         [[0.05490196, 0.12156863, 0.05882353],\n",
       "          [0.05490196, 0.12156863, 0.05882353],\n",
       "          [0.05882353, 0.1254902 , 0.0627451 ],\n",
       "          ...,\n",
       "          [0.21960784, 0.25490198, 0.24313726],\n",
       "          [0.1764706 , 0.21176471, 0.20784314],\n",
       "          [0.15686275, 0.19215687, 0.1882353 ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.07450981, 0.08627451, 0.04313726],\n",
       "          [0.07450981, 0.09019608, 0.04705882],\n",
       "          [0.08235294, 0.09411765, 0.05098039],\n",
       "          ...,\n",
       "          [0.07450981, 0.15686275, 0.1764706 ],\n",
       "          [0.09803922, 0.18039216, 0.2       ],\n",
       "          [0.10588235, 0.19215687, 0.20784314]],\n",
       " \n",
       "         [[0.0627451 , 0.08235294, 0.03921569],\n",
       "          [0.07058824, 0.08627451, 0.04313726],\n",
       "          [0.07843138, 0.09019608, 0.04705882],\n",
       "          ...,\n",
       "          [0.05098039, 0.13333334, 0.15294118],\n",
       "          [0.05098039, 0.13333334, 0.15294118],\n",
       "          [0.05098039, 0.13333334, 0.15294118]],\n",
       " \n",
       "         [[0.05882353, 0.08235294, 0.03529412],\n",
       "          [0.0627451 , 0.08627451, 0.03921569],\n",
       "          [0.07450981, 0.09019608, 0.04705882],\n",
       "          ...,\n",
       "          [0.03137255, 0.11372549, 0.13333334],\n",
       "          [0.01960784, 0.10196079, 0.12156863],\n",
       "          [0.01568628, 0.09803922, 0.11764706]]],\n",
       " \n",
       " \n",
       "        [[[0.3372549 , 0.27450982, 0.21568628],\n",
       "          [0.3529412 , 0.2901961 , 0.23137255],\n",
       "          [0.36862746, 0.3019608 , 0.24313726],\n",
       "          ...,\n",
       "          [0.42352942, 0.3254902 , 0.29803923],\n",
       "          [0.41568628, 0.31764707, 0.2901961 ],\n",
       "          [0.40392157, 0.30588236, 0.2784314 ]],\n",
       " \n",
       "         [[0.3372549 , 0.27450982, 0.21568628],\n",
       "          [0.34901962, 0.28627452, 0.22745098],\n",
       "          [0.3647059 , 0.3019608 , 0.23921569],\n",
       "          ...,\n",
       "          [0.40392157, 0.30588236, 0.2784314 ],\n",
       "          [0.40392157, 0.30588236, 0.2784314 ],\n",
       "          [0.39607844, 0.29803923, 0.27058825]],\n",
       " \n",
       "         [[0.34509805, 0.28235295, 0.21960784],\n",
       "          [0.35686275, 0.2901961 , 0.22745098],\n",
       "          [0.36862746, 0.3019608 , 0.23921569],\n",
       "          ...,\n",
       "          [0.35686275, 0.25882354, 0.23137255],\n",
       "          [0.36862746, 0.27058825, 0.23921569],\n",
       "          [0.37254903, 0.2784314 , 0.24705882]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.41960785, 0.28235295, 0.21176471],\n",
       "          [0.40784314, 0.26666668, 0.19215687],\n",
       "          [0.38039216, 0.24705882, 0.17254902],\n",
       "          ...,\n",
       "          [0.29803923, 0.17254902, 0.11372549],\n",
       "          [0.29411766, 0.16862746, 0.10980392],\n",
       "          [0.29411766, 0.16862746, 0.10980392]],\n",
       " \n",
       "         [[0.44313726, 0.30588236, 0.23137255],\n",
       "          [0.43529412, 0.29803923, 0.22352941],\n",
       "          [0.41568628, 0.2784314 , 0.20392157],\n",
       "          ...,\n",
       "          [0.29411766, 0.16862746, 0.10980392],\n",
       "          [0.2901961 , 0.16470589, 0.10588235],\n",
       "          [0.28627452, 0.16078432, 0.10196079]],\n",
       " \n",
       "         [[0.45490196, 0.3137255 , 0.2509804 ],\n",
       "          [0.45490196, 0.3137255 , 0.2509804 ],\n",
       "          [0.4509804 , 0.30980393, 0.23921569],\n",
       "          ...,\n",
       "          [0.29411766, 0.16862746, 0.10980392],\n",
       "          [0.29411766, 0.16862746, 0.10588235],\n",
       "          [0.2901961 , 0.16470589, 0.10588235]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[0.46666667, 0.3647059 , 0.29803923],\n",
       "          [0.4       , 0.29803923, 0.23137255],\n",
       "          [0.32941177, 0.22745098, 0.15686275],\n",
       "          ...,\n",
       "          [0.11372549, 0.07058824, 0.02745098],\n",
       "          [0.1254902 , 0.07450981, 0.03921569],\n",
       "          [0.13333334, 0.08235294, 0.04705882]],\n",
       " \n",
       "         [[0.40392157, 0.3019608 , 0.23529412],\n",
       "          [0.34117648, 0.24313726, 0.1764706 ],\n",
       "          [0.28627452, 0.18039216, 0.11372549],\n",
       "          ...,\n",
       "          [0.11372549, 0.06666667, 0.02745098],\n",
       "          [0.11764706, 0.0627451 , 0.02745098],\n",
       "          [0.11764706, 0.06666667, 0.03137255]],\n",
       " \n",
       "         [[0.36078432, 0.25882354, 0.19215687],\n",
       "          [0.3137255 , 0.21176471, 0.14509805],\n",
       "          [0.27058825, 0.16862746, 0.10196079],\n",
       "          ...,\n",
       "          [0.12156863, 0.07450981, 0.03137255],\n",
       "          [0.11372549, 0.05882353, 0.02352941],\n",
       "          [0.10980392, 0.05490196, 0.01960784]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.62352943, 0.4627451 , 0.2509804 ],\n",
       "          [0.62352943, 0.4627451 , 0.24705882],\n",
       "          [0.627451  , 0.4627451 , 0.2509804 ],\n",
       "          ...,\n",
       "          [0.6392157 , 0.4627451 , 0.30980393],\n",
       "          [0.6313726 , 0.4509804 , 0.29803923],\n",
       "          [0.6313726 , 0.4509804 , 0.29803923]],\n",
       " \n",
       "         [[0.6313726 , 0.4745098 , 0.24705882],\n",
       "          [0.627451  , 0.47058824, 0.24313726],\n",
       "          [0.6313726 , 0.47058824, 0.24705882],\n",
       "          ...,\n",
       "          [0.63529414, 0.45882353, 0.30588236],\n",
       "          [0.627451  , 0.4509804 , 0.29803923],\n",
       "          [0.627451  , 0.4509804 , 0.29803923]],\n",
       " \n",
       "         [[0.6392157 , 0.48235294, 0.2509804 ],\n",
       "          [0.6313726 , 0.4745098 , 0.24313726],\n",
       "          [0.6313726 , 0.47058824, 0.24705882],\n",
       "          ...,\n",
       "          [0.63529414, 0.45882353, 0.30588236],\n",
       "          [0.627451  , 0.4509804 , 0.29803923],\n",
       "          [0.627451  , 0.4509804 , 0.29803923]]],\n",
       " \n",
       " \n",
       "        [[[0.11764706, 0.10980392, 0.02745098],\n",
       "          [0.10980392, 0.10196079, 0.01960784],\n",
       "          [0.11764706, 0.09803922, 0.02352941],\n",
       "          ...,\n",
       "          [0.02352941, 0.03529412, 0.        ],\n",
       "          [0.02745098, 0.03137255, 0.        ],\n",
       "          [0.02352941, 0.02745098, 0.        ]],\n",
       " \n",
       "         [[0.10980392, 0.09803922, 0.02352941],\n",
       "          [0.10588235, 0.09019608, 0.01568628],\n",
       "          [0.10980392, 0.09019608, 0.01568628],\n",
       "          ...,\n",
       "          [0.02352941, 0.03529412, 0.        ],\n",
       "          [0.02745098, 0.03137255, 0.        ],\n",
       "          [0.02352941, 0.02745098, 0.        ]],\n",
       " \n",
       "         [[0.10588235, 0.08235294, 0.01960784],\n",
       "          [0.10196079, 0.07450981, 0.01176471],\n",
       "          [0.10588235, 0.07843138, 0.01568628],\n",
       "          ...,\n",
       "          [0.02352941, 0.03529412, 0.        ],\n",
       "          [0.02745098, 0.03137255, 0.        ],\n",
       "          [0.02352941, 0.02745098, 0.        ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.31764707, 0.1882353 , 0.16862746],\n",
       "          [0.3137255 , 0.18431373, 0.16470589],\n",
       "          [0.30980393, 0.18039216, 0.16078432],\n",
       "          ...,\n",
       "          [0.2627451 , 0.14901961, 0.09411765],\n",
       "          [0.27058825, 0.16078432, 0.11764706],\n",
       "          [0.27450982, 0.16470589, 0.12156863]],\n",
       " \n",
       "         [[0.3372549 , 0.14509805, 0.15686275],\n",
       "          [0.3137255 , 0.14901961, 0.15294118],\n",
       "          [0.28235295, 0.14509805, 0.13725491],\n",
       "          ...,\n",
       "          [0.2784314 , 0.15294118, 0.10980392],\n",
       "          [0.28627452, 0.16862746, 0.13725491],\n",
       "          [0.28235295, 0.16470589, 0.13333334]],\n",
       " \n",
       "         [[0.27450982, 0.08235294, 0.09411765],\n",
       "          [0.23529412, 0.07058824, 0.07450981],\n",
       "          [0.20784314, 0.07450981, 0.0627451 ],\n",
       "          ...,\n",
       "          [0.2901961 , 0.16470589, 0.12156863],\n",
       "          [0.28235295, 0.16470589, 0.13333334],\n",
       "          [0.29411766, 0.1764706 , 0.14509805]]],\n",
       " \n",
       " \n",
       "        [[[0.47843137, 0.5137255 , 0.49411765],\n",
       "          [0.47843137, 0.50980395, 0.49411765],\n",
       "          [0.4862745 , 0.50980395, 0.49411765],\n",
       "          ...,\n",
       "          [0.04705882, 0.04313726, 0.0627451 ],\n",
       "          [0.04705882, 0.04313726, 0.0627451 ],\n",
       "          [0.04705882, 0.04313726, 0.0627451 ]],\n",
       " \n",
       "         [[0.4745098 , 0.50980395, 0.49019608],\n",
       "          [0.47843137, 0.50980395, 0.49019608],\n",
       "          [0.48235294, 0.5058824 , 0.49019608],\n",
       "          ...,\n",
       "          [0.04705882, 0.04313726, 0.0627451 ],\n",
       "          [0.04705882, 0.04313726, 0.0627451 ],\n",
       "          [0.04705882, 0.04313726, 0.0627451 ]],\n",
       " \n",
       "         [[0.4745098 , 0.50980395, 0.49019608],\n",
       "          [0.4745098 , 0.5058824 , 0.49019608],\n",
       "          [0.47843137, 0.5019608 , 0.4862745 ],\n",
       "          ...,\n",
       "          [0.04705882, 0.04313726, 0.0627451 ],\n",
       "          [0.04705882, 0.04313726, 0.0627451 ],\n",
       "          [0.04705882, 0.04313726, 0.0627451 ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.3372549 , 0.38039216, 0.3647059 ],\n",
       "          [0.34117648, 0.38431373, 0.36862746],\n",
       "          [0.34901962, 0.39215687, 0.3764706 ],\n",
       "          ...,\n",
       "          [0.27058825, 0.21960784, 0.19607843],\n",
       "          [0.27450982, 0.22352941, 0.2       ],\n",
       "          [0.2784314 , 0.22745098, 0.2       ]],\n",
       " \n",
       "         [[0.34509805, 0.38431373, 0.37254903],\n",
       "          [0.3529412 , 0.39215687, 0.3764706 ],\n",
       "          [0.36078432, 0.4       , 0.3882353 ],\n",
       "          ...,\n",
       "          [0.41568628, 0.3647059 , 0.34509805],\n",
       "          [0.42352942, 0.37254903, 0.3529412 ],\n",
       "          [0.42745098, 0.38039216, 0.35686275]],\n",
       " \n",
       "         [[0.35686275, 0.39215687, 0.38039216],\n",
       "          [0.3647059 , 0.4       , 0.3882353 ],\n",
       "          [0.3764706 , 0.4117647 , 0.4       ],\n",
       "          ...,\n",
       "          [0.45882353, 0.41960785, 0.40392157],\n",
       "          [0.46666667, 0.42745098, 0.4117647 ],\n",
       "          [0.47058824, 0.43529412, 0.41568628]]]], dtype=float32),\n",
       " array([[0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.]]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation Data - This data can be used to test the model processing.\n",
    "valid_x = resize_images(train[501:1001], train_img_dir)\n",
    "valid_y = encode_feature(train['Class'][501:1001])\n",
    "(valid_x, valid_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Benchmark Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, InputLayer\n",
    "\n",
    "# Variables & Parameters\n",
    "input_units = (64, 64, 3)\n",
    "hidden_units = 500\n",
    "output_units = 3\n",
    "epochs = 20\n",
    "batch_size = 2000\n",
    "\n",
    "# Model Creation\n",
    "benchmark_model = Sequential([\n",
    "                    InputLayer(input_shape=input_units),\n",
    "                    Flatten(),\n",
    "                    Dense(units=hidden_units, activation='relu'),\n",
    "                    Dense(units=output_units, activation='softmax'),\n",
    "                  ])\n",
    "\n",
    "# Model Summary\n",
    "benchmark_model.summary()\n",
    "\n",
    "# Model Compilation\n",
    "benchmark_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model Training & Validation\n",
    "benchmark_model.fit(train_x, train_y[:10001], batch_size=batch_size,epochs=epochs,verbose=1,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a prediction on the test set\n",
    "test_predictions = model.predict(x_test)\n",
    "test_predictions = np.round(test_predictions)# benchmark_model.evaluate(test_x, test_y[:10001], batch_size=batch_size,epochs=epochs,verbose=1)\n",
    "\n",
    "# sample models\n",
    "\n",
    "benchmark_model.predict_classes()\n",
    "\n",
    "# model.predict_proba()\n",
    "# evaluate the model\n",
    "# scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "# print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "# cvscores.append(scores[1] * 100)\n",
    "\n",
    "benchmark_model.metric_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Implementation\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Variables & Parameters\n",
    "batch_size = 500\n",
    "num_classes = 3\n",
    "epochs = 30\n",
    "input_shape= (64, 64, 3)\n",
    "\n",
    "# Model Creation\n",
    "model = Sequential()\n",
    "model.add(Conv2D(100, kernel_size=(2,2), activation='relu', padding='same', input_shape=input_shape ))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(200, kernel_size=(2,2), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(300, kernel_size=(2,2), activation='relu', padding='same' ))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Model Summary\n",
    "model.summary()\n",
    "\n",
    "# Model Compilation\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model Training & Validation\n",
    "model.fit(train_x, train_y[:10001], batch_size=batch_size,epochs=epochs,verbose=1,shuffle=True,validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "\n",
    "=================================================================\n",
    "\n",
    "conv2d_1 (Conv2D)            (None, 64, 64, 100)       1300      \n",
    "_________________________________________________________________\n",
    "batch_normalization_1 (Batch (None, 64, 64, 100)       400       \n",
    "_________________________________________________________________\n",
    "conv2d_2 (Conv2D)            (None, 64, 64, 200)       80200     \n",
    "_________________________________________________________________\n",
    "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 200)       0         \n",
    "_________________________________________________________________\n",
    "batch_normalization_2 (Batch (None, 32, 32, 200)       800       \n",
    "_________________________________________________________________\n",
    "conv2d_3 (Conv2D)            (None, 32, 32, 300)       240300    \n",
    "_________________________________________________________________\n",
    "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 300)       0         \n",
    "_________________________________________________________________\n",
    "batch_normalization_3 (Batch (None, 16, 16, 300)       1200      \n",
    "_________________________________________________________________\n",
    "flatten_3 (Flatten)          (None, 76800)             0         \n",
    "_________________________________________________________________\n",
    "dense_5 (Dense)              (None, 200)               15360200  \n",
    "_________________________________________________________________\n",
    "dropout_1 (Dropout)          (None, 200)               0         \n",
    "_________________________________________________________________\n",
    "dense_6 (Dense)              (None, 30)                6030      \n",
    "_________________________________________________________________\n",
    "dense_7 (Dense)              (None, 3)                 93        \n",
    "\n",
    "=================================================================\n",
    "\n",
    "Total params: 15,690,523\n",
    "Trainable params: 15,689,323\n",
    "Non-trainable params: 1,200\n",
    "_________________________________________________________________\n",
    "\n",
    "Train on 15200 samples, validate on 3801 samples\n",
    "\n",
    "Epoch 1/30\n",
    "15200/15200 [==============================] - 78s 5ms/step - loss: 1.0668 - acc: 0.5914 - val_loss: 0.7805 - val_acc: 0.6435\n",
    "\n",
    "Epoch 2/30\n",
    "15200/15200 [==============================] - 68s 4ms/step - loss: 0.7265 - acc: 0.6763 - val_loss: 0.6796 - val_acc: 0.7093\n",
    "\n",
    "Epoch 3/30\n",
    "15200/15200 [==============================] - 68s 4ms/step - loss: 0.6702 - acc: 0.7046 - val_loss: 0.6793 - val_acc: 0.7096\n",
    "\n",
    "Epoch 4/30\n",
    "15200/15200 [==============================] - 69s 5ms/step - loss: 0.6254 - acc: 0.7312 - val_loss: 0.6607 - val_acc: 0.7219\n",
    "\n",
    "Epoch 5/30\n",
    "15200/15200 [==============================] - 68s 4ms/step - loss: 0.5800 - acc: 0.7564 - val_loss: 0.6170 - val_acc: 0.7459\n",
    "\n",
    "Epoch 6/30\n",
    "15200/15200 [==============================] - 68s 4ms/step - loss: 0.5428 - acc: 0.7760 - val_loss: 0.5940 - val_acc: 0.7580\n",
    "\n",
    "Epoch 7/30\n",
    "15200/15200 [==============================] - 68s 5ms/step - loss: 0.5015 - acc: 0.7963 - val_loss: 0.6198 - val_acc: 0.7440\n",
    "\n",
    "Epoch 8/30\n",
    "15200/15200 [==============================] - 68s 4ms/step - loss: 0.4661 - acc: 0.8140 - val_loss: 0.5746 - val_acc: 0.7622\n",
    "\n",
    "Epoch 9/30\n",
    "15200/15200 [==============================] - 68s 4ms/step - loss: 0.4375 - acc: 0.8249 - val_loss: 0.5753 - val_acc: 0.7574\n",
    "\n",
    "Epoch 10/30\n",
    "15200/15200 [==============================] - 68s 4ms/step - loss: 0.4039 - acc: 0.8425 - val_loss: 0.5820 - val_acc: 0.7632\n",
    "\n",
    "Epoch 11/30\n",
    "15200/15200 [==============================] - 68s 4ms/step - loss: 0.3775 - acc: 0.8561 - val_loss: 0.5629 - val_acc: 0.7748\n",
    "\n",
    "Epoch 12/30\n",
    "15200/15200 [==============================] - 68s 4ms/step - loss: 0.3515 - acc: 0.8653 - val_loss: 0.5582 - val_acc: 0.7748\n",
    "\n",
    "Epoch 13/30\n",
    "15200/15200 [==============================] - 68s 5ms/step - loss: 0.3252 - acc: 0.8797 - val_loss: 0.5867 - val_acc: 0.7669\n",
    "\n",
    "Epoch 14/30\n",
    "15200/15200 [==============================] - 68s 4ms/step - loss: 0.2998 - acc: 0.8880 - val_loss: 0.6146 - val_acc: 0.7543\n",
    "\n",
    "Epoch 15/30\n",
    "15200/15200 [==============================] - 68s 4ms/step - loss: 0.2726 - acc: 0.9035 - val_loss: 0.5536 - val_acc: 0.7824\n",
    "\n",
    "Epoch 16/30\n",
    "15200/15200 [==============================] - 68s 4ms/step - loss: 0.2598 - acc: 0.9080 - val_loss: 0.5524 - val_acc: 0.7816\n",
    "\n",
    "Epoch 17/30\n",
    "15200/15200 [==============================] - 68s 4ms/step - loss: 0.2300 - acc: 0.9232 - val_loss: 0.5337 - val_acc: 0.7922\n",
    "\n",
    "Epoch 18/30\n",
    "15200/15200 [==============================] - 68s 4ms/step - loss: 0.2150 - acc: 0.9303 - val_loss: 0.5788 - val_acc: 0.7764\n",
    "\n",
    "Epoch 19/30\n",
    "15200/15200 [==============================] - 68s 4ms/step - loss: 0.2032 - acc: 0.9338 - val_loss: 0.5364 - val_acc: 0.7916\n",
    "\n",
    "Epoch 20/30\n",
    "15200/15200 [==============================] - 68s 4ms/step - loss: 0.1766 - acc: 0.9453 - val_loss: 0.5596 - val_acc: 0.7845\n",
    "\n",
    "Epoch 21/30\n",
    "15200/15200 [==============================] - 68s 4ms/step - loss: 0.1655 - acc: 0.9495 - val_loss: 0.5438 - val_acc: 0.7895\n",
    "\n",
    "Epoch 22/30\n",
    "15200/15200 [==============================] - 68s 4ms/step - loss: 0.1528 - acc: 0.9541 - val_loss: 0.5367 - val_acc: 0.7872\n",
    "\n",
    "Epoch 23/30\n",
    "15200/15200 [==============================] - 68s 4ms/step - loss: 0.1434 - acc: 0.9572 - val_loss: 0.5527 - val_acc: 0.7932\n",
    "\n",
    "Epoch 24/30\n",
    "15200/15200 [==============================] - 68s 4ms/step - loss: 0.1313 - acc: 0.9640 - val_loss: 0.5761 - val_acc: 0.7914\n",
    "\n",
    "Epoch 25/30\n",
    "15200/15200 [==============================] - 68s 4ms/step - loss: 0.1224 - acc: 0.9688 - val_loss: 0.5619 - val_acc: 0.7998\n",
    "\n",
    "Epoch 26/30\n",
    "15200/15200 [==============================] - 68s 4ms/step - loss: 0.1072 - acc: 0.9728 - val_loss: 0.5761 - val_acc: 0.7958\n",
    "\n",
    "Epoch 27/30\n",
    "15200/15200 [==============================] - 68s 4ms/step - loss: 0.1017 - acc: 0.9748 - val_loss: 0.5803 - val_acc: 0.7958\n",
    "\n",
    "Epoch 28/30\n",
    "15200/15200 [==============================] - 68s 4ms/step - loss: 0.0944 - acc: 0.9787 - val_loss: 0.5775 - val_acc: 0.7977\n",
    "\n",
    "Epoch 29/30\n",
    "15200/15200 [==============================] - 68s 4ms/step - loss: 0.0872 - acc: 0.9796 - val_loss: 0.5890 - val_acc: 0.7961\n",
    "\n",
    "Epoch 30/30\n",
    "15200/15200 [==============================] - 68s 5ms/step - loss: 0.0826 - acc: 0.9804 - val_loss: 0.6048 - val_acc: 0.7906"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 2, 2, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 14,716,227\n",
      "Trainable params: 1,539\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Transfer Learning Implementation using VGG16\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from keras import optimizers, models, activations, losses\n",
    "from keras.layers import Convolution2D, Dense, Input, Flatten, Dropout, MaxPooling2D, BatchNormalization, \\\n",
    "GlobalAveragePooling2D, Concatenate\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Variables & Parameters\n",
    "input_shape= (64, 64, 3)\n",
    "\n",
    "# Import and make VGG16 model trainable = False and include_top = False to remove the top few layers\n",
    "vgg16_base_model =  applications.VGG16(weights = 'imagenet',\n",
    "                                            include_top = False,\n",
    "                                            input_shape = input_shape)\n",
    "vgg16_base_model.trainable = False\n",
    "\n",
    "# Customise by adding Dense layers on top to fit our cause\n",
    "custom_vgg16_model = Sequential()\n",
    "custom_vgg16_model.add(vgg16_base_model)\n",
    "custom_vgg16_model.add(GlobalAveragePooling2D())\n",
    "custom_vgg16_model.add(Dropout(0.2))\n",
    "custom_vgg16_model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Model Compilation\n",
    "custom_vgg16_model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=optimizers.SGD(lr=1e-4, \n",
    "                                       momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Model Summary\n",
    "custom_vgg16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
    "58892288/58889256 [==============================] - 6s 0us/step\n",
    "_________________________________________________________________\n",
    "\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "vgg16 (Model)                (None, 2, 2, 512)         14714688  \n",
    "_________________________________________________________________\n",
    "global_average_pooling2d_1 ( (None, 512)               0         \n",
    "_________________________________________________________________\n",
    "dropout_2 (Dropout)          (None, 512)               0         \n",
    "_________________________________________________________________\n",
    "dense_8 (Dense)              (None, 3)                 1539      \n",
    "\n",
    "Total params: 14,716,227\n",
    "Trainable params: 1,539\n",
    "Non-trainable params: 14,714,688\n",
    "\n",
    "_________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file = 'top_weights.hd5'\n",
    "checkpoint = ModelCheckpoint(weights_file, monitor='acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks = [checkpoint]\n",
    "batch_size = 50\n",
    "epochs = 10\n",
    "\n",
    "# Model Training\n",
    "custom_vgg16_model.fit(train_x, train_y[:19001], batch_size=batch_size,epochs=epochs,verbose=1,shuffle=True,validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on 15200 samples, validate on 3801 samples\n",
    "Epoch 1/20\n",
    "15200/15200 [==============================] - 19s 1ms/step - loss: 0.9302 - acc: 0.5786 - val_loss: 0.8996 - val_acc: 0.5925\n",
    "Epoch 2/20\n",
    "15200/15200 [==============================] - 15s 962us/step - loss: 0.9300 - acc: 0.5782 - val_loss: 0.8984 - val_acc: 0.5975\n",
    "Epoch 3/20\n",
    "15200/15200 [==============================] - 15s 961us/step - loss: 0.9259 - acc: 0.5834 - val_loss: 0.8973 - val_acc: 0.5985\n",
    "Epoch 4/20\n",
    "15200/15200 [==============================] - 15s 964us/step - loss: 0.9251 - acc: 0.5823 - val_loss: 0.8963 - val_acc: 0.5967\n",
    "Epoch 5/20\n",
    "15200/15200 [==============================] - 15s 962us/step - loss: 0.9268 - acc: 0.5802 - val_loss: 0.8953 - val_acc: 0.5985\n",
    "Epoch 6/20\n",
    "15200/15200 [==============================] - 15s 968us/step - loss: 0.9247 - acc: 0.5826 - val_loss: 0.8943 - val_acc: 0.5993\n",
    "Epoch 7/20\n",
    "15200/15200 [==============================] - 15s 966us/step - loss: 0.9212 - acc: 0.5839 - val_loss: 0.8934 - val_acc: 0.5988\n",
    "Epoch 8/20\n",
    "15200/15200 [==============================] - 15s 968us/step - loss: 0.9229 - acc: 0.5805 - val_loss: 0.8926 - val_acc: 0.5996\n",
    "Epoch 9/20\n",
    "15200/15200 [==============================] - 15s 966us/step - loss: 0.9262 - acc: 0.5803 - val_loss: 0.8918 - val_acc: 0.6004\n",
    "Epoch 10/20\n",
    "15200/15200 [==============================] - 15s 971us/step - loss: 0.9214 - acc: 0.5790 - val_loss: 0.8910 - val_acc: 0.6004\n",
    "Epoch 11/20\n",
    "15200/15200 [==============================] - 15s 968us/step - loss: 0.9203 - acc: 0.5831 - val_loss: 0.8902 - val_acc: 0.5996\n",
    "Epoch 12/20\n",
    "15200/15200 [==============================] - 15s 969us/step - loss: 0.9173 - acc: 0.5853 - val_loss: 0.8895 - val_acc: 0.5998\n",
    "Epoch 13/20\n",
    "15200/15200 [==============================] - 15s 966us/step - loss: 0.9206 - acc: 0.5835 - val_loss: 0.8888 - val_acc: 0.6006\n",
    "Epoch 14/20\n",
    "15200/15200 [==============================] - 15s 969us/step - loss: 0.9182 - acc: 0.5839 - val_loss: 0.8881 - val_acc: 0.6009\n",
    "Epoch 15/20\n",
    "15200/15200 [==============================] - 15s 967us/step - loss: 0.9150 - acc: 0.5861 - val_loss: 0.8873 - val_acc: 0.6022\n",
    "Epoch 16/20\n",
    "15200/15200 [==============================] - 15s 968us/step - loss: 0.9156 - acc: 0.5862 - val_loss: 0.8868 - val_acc: 0.6025\n",
    "Epoch 17/20\n",
    "15200/15200 [==============================] - 15s 965us/step - loss: 0.9116 - acc: 0.5892 - val_loss: 0.8861 - val_acc: 0.6014\n",
    "Epoch 18/20\n",
    "15200/15200 [==============================] - 15s 966us/step - loss: 0.9131 - acc: 0.5849 - val_loss: 0.8855 - val_acc: 0.6030\n",
    "Epoch 19/20\n",
    "15200/15200 [==============================] - 15s 967us/step - loss: 0.9133 - acc: 0.5885 - val_loss: 0.8848 - val_acc: 0.6022\n",
    "Epoch 20/20\n",
    "15200/15200 [==============================] - 15s 968us/step - loss: 0.9138 - acc: 0.5839 - val_loss: 0.8842 - val_acc: 0.6022"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
