{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineer Challenge\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Youâ€™ve been tasked to design a solution in a way that would aggregate the three data sources into one whilst providing a unique view of the consolidated data. Please include the technologies you would use to build it. !\n",
    "\n",
    "## Psuedo code\n",
    "\n",
    "1.  The design accompanied by an explanation for why you chose the specific technology.\n",
    "2.\tAn executable prototype of the pipeline managing the loading + aggregation of the data providing a unique view of the output. Ensuring the quality of the data and the code.\n",
    "3.\tDiscuss quickly the scalability and the deployment of the solution.\n",
    "\n",
    "## Steps\n",
    "\n",
    "I have put forward design from three perspectives taking a more generalistic approach considering the broadness of various ways of implementing the solution. We have three sources here:\n",
    "\n",
    "The database.csv has the user information - user_id is the primary key. This is a master data that needs to be loaded via batch process and ideally maintained in NOSQL table like HBASE, DYNAMODB for ease of access by the real-time streaming engine.\n",
    "\n",
    "The third_party_http_data.json is a JSON formatted data. Ideally in real-world setting this must be a client REST API request that fetches this data in real-time. The email-id is the primary key and based on that this api fetches the company profile and social presence related information for the customer. \n",
    "Real-world scenario implementation - Http Client implemented with Circuit Breaker Pattern for resilence and fast failure to avoid flooding http requests in case of HTTP Server unavailability.\n",
    "\n",
    "The message_bus.json is the real-time data in JSON format. This data must usually be produced to pub-sub management platform like Kafka or Kinesis from sources. The consumers can subscribe to the topic to consume the data in real-time. The spark-streaming can be used to process the data and enrich with user information and social presence information. Upon processing, the data can be persisted in hdfs in any format (orc, parquet etc.,) for ease of consumption from BI Tools like tableau, quicksight or for any real-time reporting platform.\n",
    "\n",
    "The detailed potential solution design is present in the SafetyCultureSolutionDesign.pptx attached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets\n",
    "\n",
    "### Message HTTP\n",
    "A 3rd party service that we use to enrich data which we extract via a HTTP request (3rd_party_data.json)!\n",
    "\n",
    "### Message BUS - Real Time data - Message driven\n",
    "A messaging system where we receive real time data from multiple sources (message_bus.json)!\n",
    "\n",
    "### Databases \n",
    "Multiple types of Databases, SQL and NoSQL. These databases also provide data that we can read and maintain. (database.csv)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application\n",
    "\n",
    "The below code requires the paths to be passed while instantiating the class. The parameters are:\n",
    "\n",
    "userInfo - The full path to database.csv\n",
    "thirdPartyInfo - The full path to 3rd_party_data.json\n",
    "messageBusInfo - The directory where message_bus.json file is present\n",
    "checkPointLocation - The path to checkpoint folder\n",
    "outputPath - The path to output folder\n",
    "\n",
    "Executing the below code will set up the class. The application instantiation code below that will pass the above parameter and start the consumer process. The final output orc file will be generated in the output path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "from pyspark import SparkContext as sc\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# SparkSession and Log4j initialization\n",
    "spark = SparkSession.builder.appName(\"SafetyCultureDataLake\").getOrCreate()\n",
    "log4jLogger = spark._jvm.org.apache.log4j\n",
    "log = log4jLogger.LogManager.getLogger(__name__)\n",
    "\n",
    "# Message bus data load\n",
    "class MessageBusStream:\n",
    "    def __init__(self,userInfo,thirdPartyInfo,messageBusInfo,checkPointLocation,outputPath):\n",
    "        userInfo=userInfo\n",
    "        thirdPartyInfo=thirdPartyInfo\n",
    "        messageBusInfo=messageBusInfo\n",
    "        checkPointLocation=checkPointLocation\n",
    "        outputPath=outputPath\n",
    "\n",
    "    # userInfoDB\n",
    "    def loadUserInfo(self,filePath, format = \"csv\"):\n",
    "        log.info(f\"loading User Info from ${filePath}\")\n",
    "        userInfoDB = spark.read.format(format).options(header=\"true\", inferSchema=\"true\").load(filePath)\n",
    "        log.info(f\"Finished loading User Info from ${filePath}\")\n",
    "        return userInfoDB\n",
    "        # produceDataToStream()\n",
    "\n",
    "    def thirdPartySocialInfo(self,filePath, format = \"json\"):\n",
    "        thirdPartyHttpFields = [StructField(\"user_email\", StringType(), True),StructField(\"industry\", StringType(), True),StructField(\"region\", StringType(), True),StructField(\"compay_profile\", StringType(), True),StructField(\"social_presence\", StringType(), True)]\n",
    "        thirdPartyHttpSchema = StructType(thirdPartyHttpFields)\n",
    "        thirdPartyHttpData = spark.read.format(format).schema(thirdPartyHttpSchema).load(filePath)\n",
    "        return thirdPartyHttpData\n",
    "\n",
    "    def produceMessageBusStream(self,filePath):\n",
    "        messageBusFields = [StructField(\"user_id\", StringType(), True),StructField(\"checklist_id\", StringType(), True),StructField(\"checklist_name\", StringType(), True),StructField(\"items\", ArrayType(MapType(StringType(), StringType()), True), True)]\n",
    "        messageBusSchema = StructType(messageBusFields)\n",
    "        messageBusData = spark.readStream.format(\"json\").schema(messageBusSchema).load(filePath)\n",
    "        return messageBusData\n",
    "\n",
    "    def enrichMessageBusData(self,userInfo, thirdPartyInfo):\n",
    "    #   Load Lookup Data\n",
    "        userInfoDB = loadUserInfo(filePath=userInfo)\n",
    "        thirdPartyHttpData = thirdPartySocialInfo(thirdPartyInfo)\n",
    "        messageBusData = produceMessageBusStream(messageBusInfo)\n",
    "        messageBusSelectedCols = messageBusData.selectExpr(\"user_id\",\"checklist_id\",\"checklist_name\",\"explode(items) as item_details\").withColumnRenamed(\"col\", \"item_details\").withColumnRenamed(\"user_id\",\"message_bus_user_id\")\n",
    "        messageBusDataWithUserInfoDF = messageBusSelectedCols.join(userInfoDB,messageBusSelectedCols.message_bus_user_id == userInfoDB.user_id,\"inner\").drop(\"user_id\")\n",
    "        messageBusDataWithUserInfoDF.createOrReplaceTempView(\"messageBusDataWithUserInfoView\")\n",
    "        messageBusDataWithUserSocialInfo = messageBusDataWithUserInfoDF.join(thirdPartyHttpData, messageBusDataWithUserInfoDF.email == thirdPartyHttpData.user_email, \"inner\")\n",
    "        return messageBusDataWithUserSocialInfo\n",
    "\n",
    "    def consumerStart(self):\n",
    "        messageBusDataWithUserSocialInfo = enrichMessageBusData(userInfo=userInfo, thirdPartyInfo=thirdPartyInfo)\n",
    "        queryUserSocialInfo = messageBusDataWithUserSocialInfo.writeStream.format(\"orc\").option(\"checkpointLocation\", checkPointLocation).option(\"header\", True).option(\"path\", outputPath).start()\n",
    "        queryUserSocialInfo.awaitTermination()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application \n",
    "\n",
    "Below code triggers the streaming application and produces output in the output folder. It monitors for new file in the messageBusInfo path and produces output in the output path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-d8a7bf884b5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Start the application\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsumerStart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-60-b15f527c79aa>\u001b[0m in \u001b[0;36mconsumerStart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mmessageBusDataWithUserSocialInfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menrichMessageBusData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muserInfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muserInfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthirdPartyInfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthirdPartyInfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mqueryUserSocialInfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessageBusDataWithUserSocialInfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"orc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"checkpointLocation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckPointLocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mqueryUserSocialInfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/Desktop/Spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Initialize the paths to the file:\n",
    "userInfo=\"<path to database.csv file>/database.csv\"\n",
    "thirdPartyInfo=\"path to 3rd party data.json file/3rd_party_data.json\"\n",
    "messageBusInfo=\"directory where the message_bus.json file will be placed\"\n",
    "checkPointLocation=\"<random local directory in drive for checkpointing>\"\n",
    "outputPath=\"<path where the output file will be produced>\"\n",
    "\n",
    "# Instantiate the application\n",
    "app = MessageBusStream(userInfo,thirdPartyInfo,messageBusInfo,checkPointLocation,outputPath)\n",
    "\n",
    "# Start the application\n",
    "app.consumerStart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "### Note: The output above is due to keyboard interruption to stop the streaming job as it continuously runs !!\n",
    "\n",
    "The results are captured in orc format. Spark has inbuilt utility to read the orc file into dataframe and this was used for this exercise. Ideally, this can be consumed using BI tools. Please replace the name of the file in the below code to get the output.\n",
    "\n",
    "### Note: The final_output.orc file attached with the results has the output that i got. Kindly place that file in local drive and provide the path below to that file to see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(message_bus_user_id='ffbf731b-897f-465e-b4c7-2f5efa54989d', checklist_id='c9d557ee-a580-46af-91e6-60fb8ec9bc9a', checklist_name='Checklist No 243', item_details={'item_title': 'item number 0', 'item_id': 'e2953563-3c29-451a-8611-b775e4bb3190', 'item_value': '0'}, email='gumpish@yahoo.ca', name='Ayana Davidson', address='Address 16', country='Australia', city='Sydney', postcode=2000, company_name='Company_16', user_email='gumpish@yahoo.ca', industry='industry_5', region='region_1', compay_profile='profile', social_presence='text_text'),\n",
       " Row(message_bus_user_id='ffbf731b-897f-465e-b4c7-2f5efa54989d', checklist_id='35041ec8-58df-4c0e-b6f4-1d76cc7de350', checklist_name='Checklist No 561', item_details={'item_title': 'item number 11', 'item_id': '19fb9ab5-5e81-4159-8840-269438ea4f50', 'item_value': '11'}, email='gumpish@yahoo.ca', name='Ayana Davidson', address='Address 16', country='Australia', city='Sydney', postcode=2000, company_name='Company_16', user_email='gumpish@yahoo.ca', industry='industry_5', region='region_1', compay_profile='profile', social_presence='text_text'),\n",
       " Row(message_bus_user_id='ffbf731b-897f-465e-b4c7-2f5efa54989d', checklist_id='35041ec8-58df-4c0e-b6f4-1d76cc7de350', checklist_name='Checklist No 561', item_details={'item_title': 'item number 12', 'item_id': 'a00732b9-11a6-444b-aeab-61ebba5c86a7', 'item_value': '128'}, email='gumpish@yahoo.ca', name='Ayana Davidson', address='Address 16', country='Australia', city='Sydney', postcode=2000, company_name='Company_16', user_email='gumpish@yahoo.ca', industry='industry_5', region='region_1', compay_profile='profile', social_presence='text_text'),\n",
       " Row(message_bus_user_id='ffbf731b-897f-465e-b4c7-2f5efa54989d', checklist_id='35041ec8-58df-4c0e-b6f4-1d76cc7de350', checklist_name='Checklist No 561', item_details={'item_title': 'item number 13', 'item_id': 'e5faad07-499f-4db3-8057-151a8b73009b', 'item_value': '97'}, email='gumpish@yahoo.ca', name='Ayana Davidson', address='Address 16', country='Australia', city='Sydney', postcode=2000, company_name='Company_16', user_email='gumpish@yahoo.ca', industry='industry_5', region='region_1', compay_profile='profile', social_presence='text_text'),\n",
       " Row(message_bus_user_id='ffbf731b-897f-465e-b4c7-2f5efa54989d', checklist_id='35041ec8-58df-4c0e-b6f4-1d76cc7de350', checklist_name='Checklist No 561', item_details={'item_title': 'item number 14', 'item_id': 'c54dfa21-f36d-465c-8f36-60c5ce8e1399', 'item_value': '149'}, email='gumpish@yahoo.ca', name='Ayana Davidson', address='Address 16', country='Australia', city='Sydney', postcode=2000, company_name='Company_16', user_email='gumpish@yahoo.ca', industry='industry_5', region='region_1', compay_profile='profile', social_presence='text_text'),\n",
       " Row(message_bus_user_id='ffbf731b-897f-465e-b4c7-2f5efa54989d', checklist_id='35041ec8-58df-4c0e-b6f4-1d76cc7de350', checklist_name='Checklist No 561', item_details={'item_title': 'item number 15', 'item_id': '63d7f649-c8d1-4fd6-8928-01f7349a8406', 'item_value': '223'}, email='gumpish@yahoo.ca', name='Ayana Davidson', address='Address 16', country='Australia', city='Sydney', postcode=2000, company_name='Company_16', user_email='gumpish@yahoo.ca', industry='industry_5', region='region_1', compay_profile='profile', social_presence='text_text'),\n",
       " Row(message_bus_user_id='ffbf731b-897f-465e-b4c7-2f5efa54989d', checklist_id='413396b6-2255-4b83-9320-ed16b9dd5729', checklist_name='Checklist No 554', item_details={'item_title': 'item number 16', 'item_id': 'ef8a4a21-99b6-4614-ae65-100ff1ac894c', 'item_value': '16'}, email='gumpish@yahoo.ca', name='Ayana Davidson', address='Address 16', country='Australia', city='Sydney', postcode=2000, company_name='Company_16', user_email='gumpish@yahoo.ca', industry='industry_5', region='region_1', compay_profile='profile', social_presence='text_text'),\n",
       " Row(message_bus_user_id='ffbf731b-897f-465e-b4c7-2f5efa54989d', checklist_id='413396b6-2255-4b83-9320-ed16b9dd5729', checklist_name='Checklist No 554', item_details={'item_title': 'item number 17', 'item_id': '142e12e4-b9ed-4972-bfea-8e14e270d4e8', 'item_value': '94'}, email='gumpish@yahoo.ca', name='Ayana Davidson', address='Address 16', country='Australia', city='Sydney', postcode=2000, company_name='Company_16', user_email='gumpish@yahoo.ca', industry='industry_5', region='region_1', compay_profile='profile', social_presence='text_text'),\n",
       " Row(message_bus_user_id='ffbf731b-897f-465e-b4c7-2f5efa54989d', checklist_id='413396b6-2255-4b83-9320-ed16b9dd5729', checklist_name='Checklist No 554', item_details={'item_title': 'item number 18', 'item_id': '9bf0fd17-6d9b-44ab-9eb0-bb6981428256', 'item_value': '144'}, email='gumpish@yahoo.ca', name='Ayana Davidson', address='Address 16', country='Australia', city='Sydney', postcode=2000, company_name='Company_16', user_email='gumpish@yahoo.ca', industry='industry_5', region='region_1', compay_profile='profile', social_presence='text_text'),\n",
       " Row(message_bus_user_id='ffbf731b-897f-465e-b4c7-2f5efa54989d', checklist_id='413396b6-2255-4b83-9320-ed16b9dd5729', checklist_name='Checklist No 554', item_details={'item_title': 'item number 19', 'item_id': '23025f72-d045-4b16-a7a4-a9532ac9ee5b', 'item_value': '280'}, email='gumpish@yahoo.ca', name='Ayana Davidson', address='Address 16', country='Australia', city='Sydney', postcode=2000, company_name='Company_16', user_email='gumpish@yahoo.ca', industry='industry_5', region='region_1', compay_profile='profile', social_presence='text_text'),\n",
       " Row(message_bus_user_id='ffbf731b-897f-465e-b4c7-2f5efa54989d', checklist_id='413396b6-2255-4b83-9320-ed16b9dd5729', checklist_name='Checklist No 554', item_details={'item_title': 'item number 20', 'item_id': 'c33210f2-ffc8-483c-9720-bf3e5df08086', 'item_value': '284'}, email='gumpish@yahoo.ca', name='Ayana Davidson', address='Address 16', country='Australia', city='Sydney', postcode=2000, company_name='Company_16', user_email='gumpish@yahoo.ca', industry='industry_5', region='region_1', compay_profile='profile', social_presence='text_text'),\n",
       " Row(message_bus_user_id='ffbf731b-897f-465e-b4c7-2f5efa54989d', checklist_id='7e53b185-19ed-4888-a908-f96307f5ffc9', checklist_name='Checklist No 181', item_details={'item_title': 'item number 30', 'item_id': '9959b749-7ca2-439c-b2fa-65340dfa2374', 'item_value': '30'}, email='gumpish@yahoo.ca', name='Ayana Davidson', address='Address 16', country='Australia', city='Sydney', postcode=2000, company_name='Company_16', user_email='gumpish@yahoo.ca', industry='industry_5', region='region_1', compay_profile='profile', social_presence='text_text')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = spark.read.orc(\"<directory to the output orc file>/final_output.orc\")\n",
    "result.where(col(\"message_bus_user_id\") == \"ffbf731b-897f-465e-b4c7-2f5efa54989d\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
